{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessed.zip file are located in Google drive, in order to train a model preprocessed file should be created under the local disk of the colab"
      ],
      "metadata": {
        "id": "96dEdmub38pa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HfoTQufLqcem"
      },
      "outputs": [],
      "source": [
        "!cp \"/content/drive/MyDrive/wifi_csi/wifi_csi/preprocessed.zip\" /content/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/preprocessed.zip -d /content"
      ],
      "metadata": {
        "id": "cn0r9QpBqy8M"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing necessary libraries. Select a GPU for high performance"
      ],
      "metadata": {
        "id": "eXBQ95sh3xl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN+LSTM Hybrid Model With Additional Signal Processing"
      ],
      "metadata": {
        "id": "dXcmdpXtn5HJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# =============================================================================\n",
        "# 1. SETUP & CONFIGURATION\n",
        "# =============================================================================\n",
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "CONFIG = {\n",
        "    # Paths\n",
        "    'dataset_path': '/content/drive/MyDrive/wifi_csi',\n",
        "    'preprocessed_dir': '/content/preprocessed',\n",
        "    'annotation_file': '/content/annotation.csv',\n",
        "\n",
        "    # Training\n",
        "    'batch_size': 32,\n",
        "    'lr': 5e-4,\n",
        "    'epochs': 30,\n",
        "    'num_classes': 6,\n",
        "    'window': 1500,  # Time window to use\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # Augmentation\n",
        "    'use_augmentation': True,\n",
        "    'mixup_alpha': 0.2,\n",
        "    'time_mask_ratio': 0.1,  # Mask 10% of time steps\n",
        "    'freq_mask_ratio': 0.1,  # Mask 10% of frequency bins\n",
        "\n",
        "    # Domain Adaptation\n",
        "    'use_coral': True,\n",
        "    'coral_weight': 0.5,\n",
        "\n",
        "    # Regularization\n",
        "    'label_smoothing': 0.1,\n",
        "    'dropout': 0.4,\n",
        "    'weight_decay': 1e-4,\n",
        "}\n",
        "\n",
        "print(f\"üöÄ Running on Device: {CONFIG['device']}\")\n",
        "\n",
        "# =============================================================================\n",
        "# 2. DATASET CLASS (FIXED FOR YOUR DATA FORMAT)\n",
        "# =============================================================================\n",
        "\n",
        "class WimansDatasetFixed(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class compatible with preprocessed data format: (2, 3000, 270)\n",
        "    - Channel 0: Amplitude\n",
        "    - Channel 1: Phase\n",
        "    - 270 = 9 antenna pairs √ó 30 subcarriers (flattened)\n",
        "    \"\"\"\n",
        "    def __init__(self, df, root_dir, config, is_train=False):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.root_dir = root_dir\n",
        "        self.config = config\n",
        "        self.is_train = is_train\n",
        "        self.window = config['window']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        file_path = os.path.join(self.root_dir, f\"{row['label']}.npy\")\n",
        "        label = row['number_of_users']\n",
        "\n",
        "        try:\n",
        "            # Load preprocessed data: (2, 3000, 270)\n",
        "            data = np.load(file_path).astype(np.float32)\n",
        "\n",
        "            # Validate shape\n",
        "            if data.shape[0] != 2:\n",
        "                raise ValueError(f\"Expected 2 channels, got {data.shape[0]}\")\n",
        "\n",
        "            total_time = data.shape[1]  # Should be 3000\n",
        "            n_features = data.shape[2]  # Should be 270\n",
        "\n",
        "            # Time slicing\n",
        "            if self.is_train:\n",
        "                # Random slice during training\n",
        "                if total_time > self.window:\n",
        "                    start = np.random.randint(0, total_time - self.window)\n",
        "                else:\n",
        "                    start = 0\n",
        "            else:\n",
        "                # Center slice during validation/test\n",
        "                start = max(0, (total_time - self.window) // 2)\n",
        "\n",
        "            # Handle sequences shorter than window\n",
        "            if total_time < self.window:\n",
        "                pad_amt = self.window - total_time\n",
        "                data = np.pad(data, ((0, 0), (0, pad_amt), (0, 0)), mode='edge')\n",
        "                start = 0\n",
        "\n",
        "            # Extract window: (2, window, 270)\n",
        "            data = data[:, start:start+self.window, :]\n",
        "\n",
        "            # Reshape 270 back to (9, 30) for spatial structure\n",
        "            # New shape: (2, window, 9, 30)\n",
        "            amp = data[0].reshape(self.window, 9, 30)   # (window, 9, 30)\n",
        "            phase = data[1].reshape(self.window, 9, 30) # (window, 9, 30)\n",
        "\n",
        "            # Transpose to channel-first: (9, window, 30) for each\n",
        "            amp = amp.transpose(1, 0, 2)     # (9, window, 30)\n",
        "            phase = phase.transpose(1, 0, 2) # (9, window, 30)\n",
        "\n",
        "            # Apply augmentations during training\n",
        "            if self.is_train and self.config['use_augmentation']:\n",
        "                amp, phase = self._apply_augmentations(amp, phase)\n",
        "\n",
        "            # Stack amplitude and phase: (18, window, 30)\n",
        "            processed_data = np.concatenate([amp, phase], axis=0)\n",
        "\n",
        "            # Per-sample normalization (separate for amp and phase)\n",
        "            processed_data = self._normalize(processed_data)\n",
        "\n",
        "            return torch.from_numpy(processed_data), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        except Exception as e:\n",
        "            # Return zeros on error (will be filtered by model)\n",
        "            print(f\"Error loading {file_path}: {e}\")\n",
        "            return torch.zeros(18, self.window, 30), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "    def _apply_augmentations(self, amp, phase):\n",
        "        \"\"\"Apply data augmentations.\"\"\"\n",
        "        # 1. Time masking (SpecAugment style)\n",
        "        if np.random.random() < 0.5:\n",
        "            mask_len = int(self.window * self.config['time_mask_ratio'])\n",
        "            mask_start = np.random.randint(0, self.window - mask_len)\n",
        "            amp[:, mask_start:mask_start+mask_len, :] = 0\n",
        "            phase[:, mask_start:mask_start+mask_len, :] = 0\n",
        "\n",
        "        # 2. Frequency masking\n",
        "        if np.random.random() < 0.5:\n",
        "            mask_len = int(30 * self.config['freq_mask_ratio'])\n",
        "            mask_start = np.random.randint(0, 30 - mask_len)\n",
        "            amp[:, :, mask_start:mask_start+mask_len] = 0\n",
        "            phase[:, :, mask_start:mask_start+mask_len] = 0\n",
        "\n",
        "        # 3. Random amplitude scaling (simulates AGC variation)\n",
        "        if np.random.random() < 0.5:\n",
        "            scale = np.random.uniform(0.8, 1.2)\n",
        "            amp = amp * scale\n",
        "\n",
        "        # 4. Random time reversal\n",
        "        if np.random.random() < 0.5:\n",
        "            amp = amp[:, ::-1, :].copy()\n",
        "            phase = phase[:, ::-1, :].copy()\n",
        "\n",
        "        # 5. Add small Gaussian noise\n",
        "        if np.random.random() < 0.5:\n",
        "            noise_amp = np.random.randn(*amp.shape) * 0.01\n",
        "            noise_phase = np.random.randn(*phase.shape) * 0.01\n",
        "            amp = amp + noise_amp\n",
        "            phase = phase + noise_phase\n",
        "\n",
        "        return amp, phase\n",
        "\n",
        "    def _normalize(self, data):\n",
        "        \"\"\"Normalize amplitude and phase separately.\"\"\"\n",
        "        # Amplitude: channels 0-8\n",
        "        amp = data[:9]\n",
        "        amp_mean = amp.mean()\n",
        "        amp_std = amp.std() + 1e-8\n",
        "        data[:9] = (amp - amp_mean) / amp_std\n",
        "\n",
        "        # Phase: channels 9-17\n",
        "        phase = data[9:]\n",
        "        phase_mean = phase.mean()\n",
        "        phase_std = phase.std() + 1e-8\n",
        "        data[9:] = (phase - phase_mean) / phase_std\n",
        "\n",
        "        return data.astype(np.float32)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 3. IMPROVED ARCHITECTURE WITH ATTENTION\n",
        "# =============================================================================\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    \"\"\"Squeeze-and-Excitation style channel attention.\"\"\"\n",
        "    def __init__(self, channels, reduction=4):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, channels // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channels // reduction, channels, bias=False),\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        # Average pooling path\n",
        "        avg_out = self.fc(self.avg_pool(x).view(b, c))\n",
        "        # Max pooling path\n",
        "        max_out = self.fc(self.max_pool(x).view(b, c))\n",
        "        # Combine\n",
        "        out = self.sigmoid(avg_out + max_out).view(b, c, 1, 1)\n",
        "        return x * out.expand_as(x)\n",
        "\n",
        "class ImprovedCNNLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Enhanced CNN+LSTM architecture.\n",
        "    Input: (batch, 18, time, 30) - 9 amp + 9 phase channels\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=6, input_channels=18, dropout=0.4):\n",
        "        super().__init__()\n",
        "\n",
        "        # CNN Feature Extractor\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 32, kernel_size=(7, 3), stride=(2, 1), padding=(3, 1)),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.GELU(),\n",
        "            nn.MaxPool2d((2, 1)),\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=(5, 3), stride=(2, 1), padding=(2, 1)),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.GELU(),\n",
        "            nn.MaxPool2d((2, 1)),\n",
        "        )\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "\n",
        "        # Channel Attention after conv layers\n",
        "        self.channel_attn = ChannelAttention(128)\n",
        "\n",
        "        # Adaptive pooling to collapse frequency dimension\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((None, 1))\n",
        "\n",
        "        # Layer normalization before LSTM\n",
        "        self.pre_lstm_norm = nn.LayerNorm(128)\n",
        "\n",
        "        # Bidirectional LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=128,\n",
        "            hidden_size=128,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if dropout > 0 else 0,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(256),  # 256 = 128 * 2 (bidirectional)\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout / 2),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "        # Feature output for CORAL loss\n",
        "        self.feature_dim = 256\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        # CNN forward pass\n",
        "        x = self.conv1(x)   # (B, 32, T/4, 30)\n",
        "        x = self.conv2(x)   # (B, 64, T/16, 30)\n",
        "        x = self.conv3(x)   # (B, 128, T/16, 30)\n",
        "\n",
        "        # Channel attention\n",
        "        x = self.channel_attn(x)\n",
        "\n",
        "        # Pool frequency dimension\n",
        "        x = self.adaptive_pool(x)  # (B, 128, T/16, 1)\n",
        "\n",
        "        # Reshape for LSTM: (B, T, C)\n",
        "        x = x.squeeze(3).permute(0, 2, 1)\n",
        "\n",
        "        # Layer norm\n",
        "        x = self.pre_lstm_norm(x)\n",
        "\n",
        "        # LSTM\n",
        "        self.lstm.flatten_parameters()\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Get features from last time step\n",
        "        features = lstm_out[:, -1, :]  # (B, 256)\n",
        "\n",
        "        if return_features:\n",
        "            return features\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(features)\n",
        "        return logits\n",
        "\n",
        "# =============================================================================\n",
        "# 4. LOSS FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    \"\"\"Cross entropy with label smoothing.\"\"\"\n",
        "    def __init__(self, smoothing=0.1, weight=None):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.weight = weight\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        n_classes = pred.size(-1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            smooth_labels = torch.zeros_like(pred)\n",
        "            smooth_labels.fill_(self.smoothing / (n_classes - 1))\n",
        "            smooth_labels.scatter_(1, target.unsqueeze(1), 1 - self.smoothing)\n",
        "\n",
        "        log_probs = F.log_softmax(pred, dim=-1)\n",
        "        loss = -(smooth_labels * log_probs).sum(dim=-1)\n",
        "\n",
        "        if self.weight is not None:\n",
        "            weight = self.weight[target]\n",
        "            loss = loss * weight\n",
        "\n",
        "        return loss.mean()\n",
        "\n",
        "class CORALLoss(nn.Module):\n",
        "    \"\"\"Deep CORAL loss for domain adaptation.\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, source, target):\n",
        "        d = source.size(1)\n",
        "        ns, nt = source.size(0), target.size(0)\n",
        "\n",
        "        # Source covariance\n",
        "        source_centered = source - source.mean(0, keepdim=True)\n",
        "        cs = (source_centered.T @ source_centered) / (ns - 1 + 1e-8)\n",
        "\n",
        "        # Target covariance\n",
        "        target_centered = target - target.mean(0, keepdim=True)\n",
        "        ct = (target_centered.T @ target_centered) / (nt - 1 + 1e-8)\n",
        "\n",
        "        # CORAL loss\n",
        "        loss = torch.sum((cs - ct) ** 2) / (4 * d * d)\n",
        "        return loss\n",
        "\n",
        "# =============================================================================\n",
        "# 5. MIXUP AUGMENTATION\n",
        "# =============================================================================\n",
        "\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "# =============================================================================\n",
        "# 6. TRAINING FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer, config,\n",
        "                target_loader=None, coral_loss_fn=None):\n",
        "    model.train()\n",
        "    total_loss, total_cls_loss, total_coral_loss = 0, 0, 0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    target_iter = iter(target_loader) if target_loader else None\n",
        "\n",
        "    pbar = tqdm(loader, desc=\"Training\", leave=False)\n",
        "    for x, y in pbar:\n",
        "        x, y = x.to(config['device']), y.to(config['device'])\n",
        "\n",
        "        # Skip batches with all zeros (error samples)\n",
        "        if x.abs().sum() == 0:\n",
        "            continue\n",
        "\n",
        "        # Mixup augmentation\n",
        "        use_mixup = config['use_augmentation'] and config['mixup_alpha'] > 0\n",
        "        if use_mixup and np.random.random() < 0.5:\n",
        "            x, y_a, y_b, lam = mixup_data(x, y, config['mixup_alpha'])\n",
        "        else:\n",
        "            use_mixup = False\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass with CORAL\n",
        "        if config['use_coral'] and target_loader:\n",
        "            outputs = model(x)\n",
        "            source_features = model(x, return_features=True)\n",
        "\n",
        "            try:\n",
        "                x_target, _ = next(target_iter)\n",
        "            except StopIteration:\n",
        "                target_iter = iter(target_loader)\n",
        "                x_target, _ = next(target_iter)\n",
        "\n",
        "            x_target = x_target.to(config['device'])\n",
        "            if x_target.abs().sum() > 0:  # Skip if all zeros\n",
        "                target_features = model(x_target, return_features=True)\n",
        "                coral = coral_loss_fn(source_features, target_features)\n",
        "            else:\n",
        "                coral = torch.tensor(0.0).to(config['device'])\n",
        "        else:\n",
        "            outputs = model(x)\n",
        "            coral = torch.tensor(0.0).to(config['device'])\n",
        "\n",
        "        # Classification loss\n",
        "        if use_mixup:\n",
        "            cls_loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)\n",
        "        else:\n",
        "            cls_loss = criterion(outputs, y)\n",
        "\n",
        "        # Total loss\n",
        "        loss = cls_loss + config['coral_weight'] * coral\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_cls_loss += cls_loss.item()\n",
        "        total_coral_loss += coral.item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += y.size(0)\n",
        "        if use_mixup:\n",
        "            correct += (lam * predicted.eq(y_a).float() +\n",
        "                       (1-lam) * predicted.eq(y_b).float()).sum().item()\n",
        "        else:\n",
        "            correct += predicted.eq(y).sum().item()\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{loss.item():.4f}',\n",
        "            'acc': f'{100.*correct/total:.1f}%'\n",
        "        })\n",
        "\n",
        "    n_batches = len(loader)\n",
        "    return (total_loss / n_batches, total_cls_loss / n_batches,\n",
        "            total_coral_loss / n_batches, 100. * correct / total)\n",
        "\n",
        "\n",
        "def validate(model, loader, criterion, config):\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(config['device']), y.to(config['device'])\n",
        "\n",
        "            # Skip error samples\n",
        "            if x.abs().sum() == 0:\n",
        "                continue\n",
        "\n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += y.size(0)\n",
        "            correct += predicted.eq(y).sum().item()\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "    acc = 100. * correct / total if total > 0 else 0\n",
        "    avg_loss = total_loss / len(loader) if len(loader) > 0 else 0\n",
        "    return avg_loss, acc, all_preds, all_labels\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 7. MAIN FUNCTION\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"FIXED CNN+LSTM PIPELINE FOR WIFI CSI HUMAN COUNTING\")\n",
        "    print(\"Data format: (2, 3000, 270) - preprocessed amplitude & phase\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Load annotations\n",
        "    df = pd.read_csv(CONFIG['annotation_file'])\n",
        "    print(f\"Total samples: {len(df)}\")\n",
        "    print(f\"Available Environments: {df['environment'].unique()}\")\n",
        "\n",
        "    # Domain split: train on classroom + meeting_room, test on empty_room\n",
        "    train_envs = ['classroom', 'meeting_room']\n",
        "    test_env = 'empty_room'\n",
        "\n",
        "    train_val_df = df[df['environment'].isin(train_envs)].reset_index(drop=True)\n",
        "    test_df = df[df['environment'] == test_env].reset_index(drop=True)\n",
        "\n",
        "    # Stratified validation split\n",
        "    split = StratifiedShuffleSplit(n_splits=1, test_size=0.15, random_state=42)\n",
        "    for train_idx, val_idx in split.split(train_val_df, train_val_df['number_of_users']):\n",
        "        train_df = train_val_df.iloc[train_idx]\n",
        "        val_df = train_val_df.iloc[val_idx]\n",
        "\n",
        "    print(f\"\\nDOMAIN SPLIT:\")\n",
        "    print(f\"  TRAIN: {train_envs} | {len(train_df)} samples\")\n",
        "    print(f\"  VAL:   {train_envs} | {len(val_df)} samples\")\n",
        "    print(f\"  TEST:  {test_env} | {len(test_df)} samples (UNSEEN)\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_ds = WimansDatasetFixed(train_df, CONFIG['preprocessed_dir'], CONFIG, is_train=True)\n",
        "    val_ds = WimansDatasetFixed(val_df, CONFIG['preprocessed_dir'], CONFIG, is_train=False)\n",
        "    test_ds = WimansDatasetFixed(test_df, CONFIG['preprocessed_dir'], CONFIG, is_train=False)\n",
        "    target_ds = WimansDatasetFixed(test_df, CONFIG['preprocessed_dir'], CONFIG, is_train=False)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_ds, batch_size=CONFIG['batch_size'],\n",
        "                             shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=CONFIG['batch_size'],\n",
        "                           shuffle=False, num_workers=2, pin_memory=True)\n",
        "    test_loader = DataLoader(test_ds, batch_size=CONFIG['batch_size'],\n",
        "                            shuffle=False, num_workers=2, pin_memory=True)\n",
        "    target_loader = DataLoader(target_ds, batch_size=CONFIG['batch_size'],\n",
        "                              shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # Initialize model\n",
        "    model = ImprovedCNNLSTM(\n",
        "        num_classes=CONFIG['num_classes'],\n",
        "        input_channels=18,\n",
        "        dropout=CONFIG['dropout']\n",
        "    ).to(CONFIG['device'])\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"\\nüìä Model: {total_params:,} parameters\")\n",
        "\n",
        "    # Class weights\n",
        "    class_counts = train_df['number_of_users'].value_counts().sort_index().values\n",
        "    weights = torch.FloatTensor(1.0 / class_counts).to(CONFIG['device'])\n",
        "    weights = weights / weights.sum()\n",
        "    print(f\"Class weights: {weights.cpu().numpy().round(4)}\")\n",
        "\n",
        "    # Loss and optimizer\n",
        "    criterion = LabelSmoothingCrossEntropy(\n",
        "        smoothing=CONFIG['label_smoothing'],\n",
        "        weight=weights\n",
        "    )\n",
        "    coral_loss_fn = CORALLoss() if CONFIG['use_coral'] else None\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr'],\n",
        "                           weight_decay=CONFIG['weight_decay'])\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "        optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    print(\"\\nüî• Starting Training...\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    best_val_acc = 0\n",
        "    best_test_acc = 0\n",
        "\n",
        "    for epoch in range(CONFIG['epochs']):\n",
        "        train_loss, cls_loss, coral_loss, train_acc = train_epoch(\n",
        "            model, train_loader, criterion, optimizer, CONFIG,\n",
        "            target_loader if CONFIG['use_coral'] else None,\n",
        "            coral_loss_fn\n",
        "        )\n",
        "\n",
        "        val_loss, val_acc, _, _ = validate(model, val_loader, criterion, CONFIG)\n",
        "        test_loss, test_acc, _, _ = validate(model, test_loader, criterion, CONFIG)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1:02d}/{CONFIG['epochs']} | \"\n",
        "              f\"Train: {train_acc:.1f}% | Val: {val_acc:.1f}% | \"\n",
        "              f\"Test: {test_acc:.1f}% | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "        if test_acc > best_test_acc:\n",
        "            best_test_acc = test_acc\n",
        "\n",
        "    # Final evaluation\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üèÜ RESULTS\")\n",
        "    print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
        "    print(f\"Best Test Accuracy (Unseen): {best_test_acc:.2f}%\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Load best model and evaluate\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    _, final_acc, preds, labels = validate(model, test_loader, criterion, CONFIG)\n",
        "\n",
        "    print(f\"\\nüìä Final Test Results ({test_env}):\")\n",
        "    print(classification_report(labels, preds, digits=3, zero_division=0))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9A8Cu3d0sWW",
        "outputId": "29111e22-8350-404d-c749-1f86154f4723"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Running on Device: cuda\n",
            "============================================================\n",
            "FIXED CNN+LSTM PIPELINE FOR WIFI CSI HUMAN COUNTING\n",
            "Data format: (2, 3000, 270) - preprocessed amplitude & phase\n",
            "============================================================\n",
            "Total samples: 11286\n",
            "Available Environments: ['classroom' 'meeting_room' 'empty_room']\n",
            "\n",
            "DOMAIN SPLIT:\n",
            "  TRAIN: ['classroom', 'meeting_room'] | 6395 samples\n",
            "  VAL:   ['classroom', 'meeting_room'] | 1129 samples\n",
            "  TEST:  empty_room | 3762 samples (UNSEEN)\n",
            "\n",
            "üìä Model: 819,302 parameters\n",
            "Class weights: [0.4005 0.0666 0.1332 0.1332 0.1332 0.1332]\n",
            "\n",
            "üî• Starting Training...\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/30 | Train: 33.7% | Val: 54.6% | Test: 24.0% | LR: 4.88e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 02/30 | Train: 52.8% | Val: 65.5% | Test: 30.4% | LR: 4.52e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 03/30 | Train: 60.7% | Val: 71.8% | Test: 32.7% | LR: 3.97e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 04/30 | Train: 64.3% | Val: 72.9% | Test: 27.7% | LR: 3.28e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 05/30 | Train: 67.1% | Val: 74.0% | Test: 27.9% | LR: 2.51e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 06/30 | Train: 70.1% | Val: 77.7% | Test: 32.8% | LR: 1.73e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 07/30 | Train: 71.8% | Val: 78.7% | Test: 31.8% | LR: 1.04e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 08/30 | Train: 73.2% | Val: 82.0% | Test: 37.1% | LR: 4.87e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 09/30 | Train: 77.7% | Val: 83.1% | Test: 39.4% | LR: 1.32e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/30 | Train: 77.1% | Val: 83.3% | Test: 39.2% | LR: 5.00e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/30 | Train: 70.0% | Val: 76.9% | Test: 35.4% | LR: 4.97e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/30 | Train: 71.5% | Val: 81.5% | Test: 42.7% | LR: 4.88e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/30 | Train: 73.6% | Val: 81.5% | Test: 35.5% | LR: 4.73e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/30 | Train: 73.6% | Val: 80.2% | Test: 35.0% | LR: 4.52e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/30 | Train: 74.4% | Val: 79.9% | Test: 35.8% | LR: 4.27e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/30 | Train: 76.3% | Val: 84.4% | Test: 38.3% | LR: 3.97e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/30 | Train: 75.4% | Val: 83.2% | Test: 35.1% | LR: 3.64e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/30 | Train: 77.0% | Val: 80.1% | Test: 32.9% | LR: 3.28e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/30 | Train: 79.9% | Val: 85.8% | Test: 43.6% | LR: 2.90e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/30 | Train: 79.4% | Val: 86.3% | Test: 40.5% | LR: 2.51e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/30 | Train: 80.7% | Val: 86.4% | Test: 40.3% | LR: 2.11e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/30 | Train: 81.2% | Val: 87.4% | Test: 41.3% | LR: 1.73e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/30 | Train: 82.3% | Val: 87.4% | Test: 40.4% | LR: 1.37e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/30 | Train: 82.8% | Val: 88.6% | Test: 40.5% | LR: 1.04e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/30 | Train: 84.1% | Val: 87.9% | Test: 41.9% | LR: 7.41e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/30 | Train: 83.6% | Val: 89.9% | Test: 39.8% | LR: 4.87e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/30 | Train: 84.7% | Val: 89.7% | Test: 39.8% | LR: 2.82e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/30 | Train: 86.0% | Val: 90.2% | Test: 42.1% | LR: 1.32e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/30 | Train: 87.1% | Val: 90.3% | Test: 41.5% | LR: 4.07e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/30 | Train: 86.8% | Val: 90.2% | Test: 41.6% | LR: 5.00e-04\n",
            "\n",
            "============================================================\n",
            "üèÜ RESULTS\n",
            "Best Validation Accuracy: 90.35%\n",
            "Best Test Accuracy (Unseen): 43.57%\n",
            "============================================================\n",
            "\n",
            "üìä Final Test Results (empty_room):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.818     0.500     0.621       198\n",
            "           1      0.485     0.466     0.475      1188\n",
            "           2      0.507     0.527     0.517       594\n",
            "           3      0.188     0.093     0.124       594\n",
            "           4      0.367     0.300     0.330       594\n",
            "           5      0.327     0.608     0.425       594\n",
            "\n",
            "    accuracy                          0.415      3762\n",
            "   macro avg      0.449     0.416     0.415      3762\n",
            "weighted avg      0.415     0.415     0.403      3762\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wFaSb13rUm8T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}